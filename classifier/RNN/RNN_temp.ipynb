{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json,re\n",
    "word_re = re.compile(r\"[\\w']+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import training, testing data, embedding and dictionary\n",
    "train_data = []\n",
    "test_data = []\n",
    "with open('train.txt', 'r') as f, open('test.txt', 'r') as t:\n",
    "    for line in f:\n",
    "        train_data.append(line[:-1])\n",
    "    for line in t:\n",
    "        test_data.append(line[:-1])\n",
    "\n",
    "with open('dictionary.txt', 'r') as f:\n",
    "    dictionary = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/douzhi/Software/anaconda3/envs/cs505/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2717: DtypeWarning: Columns (1,2,5,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "train_temp = pd.read_csv('train.csv')['label'].values\n",
    "test_temp = pd.read_csv('test.csv')['label'].values\n",
    "embed = pd.read_csv('final_embedding.csv').drop('Unnamed: 0', axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_label = []\n",
    "test_label = []\n",
    "for i in train_temp:\n",
    "    if i == 0:\n",
    "        train_label.append([0,1])\n",
    "    else: train_label.append([1,0])\n",
    "for i in test_temp:\n",
    "    if i == 0:\n",
    "        test_label.append([0, 1])\n",
    "    else: test_label.append([1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_batch(data, label, batch_size, indeces):\n",
    "    batch_x = []\n",
    "    batch_y = []\n",
    "    batch_l = []\n",
    "    embedLen = embed.shape[1]\n",
    "    maxlen = -1\n",
    "    for i in range(batch_size):\n",
    "        sentence = []\n",
    "        length = 0\n",
    "        for word in word_re.findall(data[i]):\n",
    "            if word.isdigit() or word == \"'\": continue\n",
    "            if word[0] == \"'\": word = word[1:]\n",
    "            if word[-1] == \"'\": word = word[:-2]\n",
    "            sentence.append(embed[dictionary.get(word.lower(), 0)])\n",
    "            length += 1\n",
    "        batch_x.append(sentence)\n",
    "        batch_y.append(label[indeces])\n",
    "        indeces = (indeces + 1) % len(data)\n",
    "        batch_l.append(length)\n",
    "        if length > maxlen: maxlen = length\n",
    "\n",
    "    # padding\n",
    "    emptyword = np.zeros(embedLen)\n",
    "    for i in range(batch_size):\n",
    "        for j in range(maxlen - len(batch_x[i])):\n",
    "            batch_x[i].append(emptyword)\n",
    "\n",
    "    return batch_x, batch_y, batch_l, indeces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_iters = 2\n",
    "batch_size = 128\n",
    "display_step = 100\n",
    "hidden_layer = 1024\n",
    "classes = 2\n",
    "embeddingLen = 128\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, None, embeddingLen], name='Inputs')\n",
    "y = tf.placeholder(\"float\", [None, classes], name='outputs')\n",
    "seqlen = tf.placeholder(tf.int32, [None])\n",
    "# Define weights\n",
    "w = tf.Variable(tf.random_normal([hidden_layer, classes]), name='weights')\n",
    "b = tf.Variable(tf.random_normal([classes]), name='bias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RNN(x, seqlen, w, b):\n",
    "    lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_layer)\n",
    "    outputs, state = tf.nn.dynamic_rnn(lstm_cell, x, seqlen, dtype=tf.float32)\n",
    "    batch_size = tf.shape(outputs)[0]\n",
    "    dataMLen = tf.shape(x)[1]\n",
    "    index = tf.range(0, batch_size) * dataMLen + (seqlen - 1)\n",
    "    output = tf.gather(tf.reshape(outputs, [-1, hidden_layer]), index)\n",
    "    output = tf.nn.softmax(tf.matmul(output, w) + b)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/douzhi/Software/anaconda3/envs/cs505/lib/python3.5/site-packages/tensorflow/python/ops/gradients.py:90: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "# set lose function\n",
    "pred = RNN(x, seqlen, w, b)\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# compute the accuracy\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  0\n",
      "Iter 0, Minibatch Loss= 0.601026, Training Accuracy= 0.71875\n",
      "iteration:  1\n",
      "0.699508415264\n"
     ]
    }
   ],
   "source": [
    "# run network\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    indeces = 0\n",
    "    for i in range(training_iters):\n",
    "        batch_x, batch_y, batch_l, indeces = generate_batch(train_data, train_label, batch_size=batch_size, indeces=indeces)\n",
    "        print(\"iteration: \", i)\n",
    "        output=sess.run(optimizer, feed_dict={x: batch_x, y: batch_y, seqlen: batch_l})\n",
    "        \n",
    "        if i % display_step == 0:\n",
    "            acc = sess.run(accuracy, feed_dict={x: batch_x, y: batch_y, seqlen: batch_l})\n",
    "            # Calculate batch loss\n",
    "            loss = sess.run(cost, feed_dict={x: batch_x, y: batch_y, seqlen: batch_l})\n",
    "            print(\"Iter \" + str(i) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.5f}\".format(acc))\n",
    "    indeces = 0\n",
    "    test_pret = []\n",
    "    for i in range(0, len(test_data), batch_size):\n",
    "        if len(test_data) - indeces < 128: batch_size = len(test_data) - indeces\n",
    "        batch_x, batch_y, batch_l, indeces = generate_batch(test_data, test_label, batch_size=batch_size, indeces=indeces)\n",
    "        test_pret.extend(sess.run(pred, feed_dict={x: batch_x, y: batch_y, seqlen: batch_l}).tolist())\n",
    "    test_pret = np.array(test_pret)\n",
    "    accu = np.equal(test_pret.argmax(1), np.array(test_label).argmax(1)).astype(np.int32)\n",
    "    accu = accu.sum() / len(accu)\n",
    "    print(accu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48008, 2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(test_label).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Required argument 'object' (pos 1) not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-2fc9ffa693e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mxx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mxx1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Required argument 'object' (pos 1) not found"
     ]
    }
   ],
   "source": [
    "xx=np.array()\n",
    "xx1=np.array([5,6])\n",
    "np.append([[1,2],[3,4]],[5,6],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xx = np.array([12])\n",
    "xx1 = [4,5,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4, 5, 6]]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:cs505]",
   "language": "python",
   "name": "conda-env-cs505-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
